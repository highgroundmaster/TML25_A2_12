{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T09:44:46.419740Z",
     "start_time": "2025-06-25T09:44:43.680448Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import base64\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f79274cc218f7ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:43:39.227640Z",
     "start_time": "2025-06-25T16:43:39.224495Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Constants\n",
    "TOKEN = \"REDACTED\"\n",
    "SEED = \"REDACTED\"\n",
    "PORT = \"REDACTED\"\n",
    "MAX_QUERIES = 10  # Max 10k images (10 queries Ã— 1000 images)\n",
    "NUM_SAMPLES = 2  # Average 2 queries per batch to reduce noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa0dc48ae4a89e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:41:32.036107Z",
     "start_time": "2025-06-25T16:41:32.032595Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Dataset Class with RGB Conversion\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a042a36621bb043e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:42:29.814381Z",
     "start_time": "2025-06-25T16:42:29.810243Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Noise-Averaged Query Function\n",
    "def get_averaged_representations(images, port, num_samples=NUM_SAMPLES):\n",
    "    all_reps = []\n",
    "    for _ in range(num_samples):\n",
    "        pil_images = [transforms.ToPILImage()(img) for img in images]\n",
    "        payload = encode_images(pil_images)\n",
    "        response = requests.get(\n",
    "            f\"http://34.122.51.94:{port}/query\",\n",
    "            files={\"file\": payload},\n",
    "            headers={\"token\": TOKEN}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            reps = torch.tensor(response.json()[\"representations\"], dtype=torch.float32)\n",
    "            all_reps.append(reps)\n",
    "        else:\n",
    "            raise Exception(f\"Query failed: {response.status_code}, {response.json()}\")\n",
    "        time.sleep(60)  # Rate limit\n",
    "    return torch.mean(torch.stack(all_reps), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1edcc25226333eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T10:00:05.944748Z",
     "start_time": "2025-06-25T10:00:05.843399Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Launch API & Load Data\n",
    "public_dataset = torch.load(\"ModelStealingPub.pt\", weights_only=False)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9867a62c5773ca7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T10:00:27.594798Z",
     "start_time": "2025-06-25T10:00:27.584434Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = TaskDataset(public_dataset.imgs, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d79e960bb0c53c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T10:24:53.420307Z",
     "start_time": "2025-06-25T10:00:29.752639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected batch 1/10\n",
      "Collected batch 2/10\n",
      "Collected batch 3/10\n",
      "Collected batch 4/10\n",
      "Collected batch 5/10\n",
      "Collected batch 6/10\n",
      "Collected batch 7/10\n",
      "Collected batch 8/10\n",
      "Collected batch 9/10\n",
      "Collected batch 10/10\n"
     ]
    }
   ],
   "source": [
    "# 6. Collect Data with Noise Averaging\n",
    "all_images = []\n",
    "all_representations = []\n",
    "\n",
    "for i in range(MAX_QUERIES):\n",
    "    start_idx = i * 1000\n",
    "    batch_images = [dataset[start_idx + j] for j in range(1000)]\n",
    "    batch_tensor = torch.stack(batch_images)  # Shape: (1000, 3, 32, 32)\n",
    "\n",
    "    # Query the same batch multiple times and average\n",
    "    avg_reps = get_averaged_representations(batch_images, PORT, num_samples=NUM_SAMPLES)\n",
    "\n",
    "    all_images.append(batch_tensor)\n",
    "    all_representations.append(avg_reps)\n",
    "    print(f\"Collected batch {i+1}/{MAX_QUERIES}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f6323bbee4954c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T10:26:00.791767Z",
     "start_time": "2025-06-25T10:26:00.761090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Flatten collected data\n",
    "all_images = torch.cat(all_images)\n",
    "all_representations = torch.cat(all_representations)\n",
    "\n",
    "# 7. Define Model and Loss\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 8 * 8, 1024)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fb2e153b3db3367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T11:35:48.079041Z",
     "start_time": "2025-06-25T11:34:15.248344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0014\n",
      "Epoch 2, Loss: 0.0014\n",
      "Epoch 3, Loss: 0.0014\n",
      "Epoch 4, Loss: 0.0013\n",
      "Epoch 5, Loss: 0.0013\n",
      "Epoch 6, Loss: 0.0012\n",
      "Epoch 7, Loss: 0.0012\n",
      "Epoch 8, Loss: 0.0012\n",
      "Epoch 9, Loss: 0.0011\n",
      "Epoch 10, Loss: 0.0011\n",
      "Epoch 11, Loss: 0.0011\n",
      "Epoch 12, Loss: 0.0010\n",
      "Epoch 13, Loss: 0.0010\n",
      "Epoch 14, Loss: 0.0011\n",
      "Epoch 15, Loss: 0.0010\n",
      "Epoch 16, Loss: 0.0010\n",
      "Epoch 17, Loss: 0.0009\n",
      "Epoch 18, Loss: 0.0009\n",
      "Epoch 19, Loss: 0.0009\n",
      "Epoch 20, Loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# 8. Train Loop\n",
    "train_dataset = torch.utils.data.TensorDataset(all_images, all_representations)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(20):  # Train for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        stolen_reps = model(images.float())  # Ensure float32\n",
    "        loss = criterion(stolen_reps, targets.float())  # Ensure targets are float32\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41a13c6e16dbe1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T11:35:51.297483Z",
     "start_time": "2025-06-25T11:35:51.258731Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"submission.onnx\"\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    path,\n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"x\": {0: \"batch_size\"}},\n",
    "    opset_version=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14fe515f52d0b0ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T11:35:57.740543Z",
     "start_time": "2025-06-25T11:35:51.872571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L2': 5.882108211517334}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10. Submit to Server\n",
    "response = requests.post(\n",
    "    \"http://34.122.51.94:9090/stealing\",\n",
    "    files={\"file\": open(path, \"rb\")},\n",
    "    headers={\"token\": TOKEN, \"seed\": SEED}\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a459da006c4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
